# Predicting Exercise Type From Movement Data

### Tom Withey

#### Executive Summary

The quantified self movement group ([click here for more details](http://groupware.les.inf.puc-rio.br/har)) has used accelorometers within devices such as fitbit to record data on barbell lifts. They were asked to perform the lifts correctly and incorrectly in 5 different ways. 

From the recorded data, provided by the quantified self movement group, I have used machine learning techniques to develop a model for predicting from the accelerometer data whether a barbell lift was undertaken correctly or incorrectly. The training data set was split into training and validation subsets and a random forest model trained using 5-fold cross validation. The model achieved an out of sample error of 2%.

#### Read the data

The data was collected from the internet and split into training and test sets.

```{r read_data}
traindata <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testdata <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainset <- read.csv(traindata,stringsAsFactors=FALSE,na.strings=c("","#DIV/0!","NA"))
testset <- read.csv(testdata,stringsAsFactors=FALSE,na.strings=c("","#DIV/0!","NA"))
```

[This paper](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) (http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) sets out the contents of the data. We know from this data that the "Classe" variable describes the five ways of carrying out the lift (of which Class A is the correct type).

#### Exploratory data analysis and data cleaning

Using only the training data set, an exploratory data analysis was undertaken. First, lets take a look at the data:

```{r inspect}
dim(trainset)
```

So there are 160 different variables, for 19,622 observations, of which one is the classification data (known as classe). Because we have so many variables, we should first concentrate on trying to reduce the number of these. A number of the variables haves NAs, so lets first clean the data by removing those variables which have large numbers of NAs or empty/blank values. Also, the x, user name, timestamp and window variables are not relevant to the results. Finally, convert the classse variable to a factor.

```{r clean, message = FALSE}
library(dplyr)
set.seed(12)
nasums <- colSums(is.na(trainset))
trainset <- trainset[,nasums==0]
trainset <- select(trainset,-c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window"))
trainset$classe <- as.factor(trainset$classe)
dim(trainset)
```

So this reduces the dataset to 53 variables, of which one is the classe result, so there are 52 predictors.

#### Creating a validation set

In order to assess out of sample error, we need to create a validation set from the training set. This is done below:

```{r val,message=FALSE}
library(caret)
inVal <- createDataPartition(y=trainset$classe,p=0.2,list=FALSE)
valset <- trainset[inVal,]
trainset <- trainset[-inVal,]
dim(valset)
dim(trainset)
```

This splits the training set into 15,696 records and the validation set into 3,927 records.

#### Fitting a model

First, there's a lot of data to process so lets do parrallel processing to speed up the run times. Also, use cross-validation within the train control.

There are 53 predictors so use principal component analysis to reduce these, then train a random forest model, using k-fold cross validation, with five folds, and check the accuracy on the training set.


```{r model1, message=FALSE}
library(parallel)
library(doParallel)
set.seed(12)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
folds = 5
fitControl1 <- trainControl(method = "cv",number = folds,allowParallel = TRUE)
modFit1 <- train(classe ~ .,data=trainset,method="rf",trControl = fitControl1,preProcess="pca")
pred1 <- predict(modFit1,trainset)
confusionMatrix(pred1,trainset$classe)
stopCluster(cluster)
registerDoSEQ()
```

With five-fold cross validation we get 100% accuracy on the training set.

#### Out of sample error 
 
Now, let's take a look at the out of sample error, by predicting on the validation set.

```{r oserror}
preds <- predict(modFit1,valset)
cM <- confusionMatrix(preds,valset$classe)
print(cM)
```

The out of sample error is one minus the accuracy, i.e. 1 - 0.98 = 2%. 
 
#### Predicting on the test set

To make predictions on the test set, we need to convert the test data to the same format as the training set. Then we use the prediction function. The results of the prediction function are supressed in this html, to maintain the honour code for the course.

```{r test_preds,results='hide'}
predictors <- names(trainset)
predictors <- predictors[-length(predictors)]
testset <- select(testset, predictors)
tpreds <- predict(modFit1,testset)
tpreds
```